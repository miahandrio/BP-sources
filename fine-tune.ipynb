{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LJtnWc3b89qn"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 1536\n",
    "MODEL_ID = \"llava-hf/llava-v1.6-vicuna-13b-hf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6XW3qVty89qq",
    "outputId": "c8ec0f67-0f56-4f16-f3f9-7af975d0904b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 01:20:00.863965: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-25 01:20:00.873235: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750807200.884902 3931450 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750807200.888500 3931450 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750807200.898982 3931450 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750807200.898994 3931450 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750807200.898996 3931450 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750807200.898997 3931450 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-25 01:20:00.902821: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\" # during training, one always uses padding on the right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHhDJ8Fs89qq"
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PCbRwGgU89qq",
    "outputId": "f3dcec0e-6b03-407c-ecee-992259c39175"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b15ac57051a47959ec262250b2afc09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "USE_LORA = False\n",
    "USE_QLORA = False\n",
    "\n",
    "## Load model\n",
    "\n",
    "# Three options for training, from the lowest precision training to the highest precision training:\n",
    "# - QLora\n",
    "# - Standard Lora\n",
    "# - Full fine-tuning\n",
    "if USE_QLORA or USE_LORA:\n",
    "    if USE_QLORA:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "    model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "else:\n",
    "    # for full fine-tuning, we can speed up the model using Flash Attention\n",
    "    # only available on certain devices, see https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features\n",
    "    model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        _attn_implementation=\"flash_attention_2\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8P6BtgGz89qq"
   },
   "source": [
    "## Apply PEFT\n",
    "\n",
    "After loading the base model, we're going to add LoRa adapter layers. We're going to only train these adapter layers (the base model is kept frozen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vHMXc9Xb89qq"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    multimodal_keywords = ['multi_modal_projector', 'vision_model']\n",
    "    for name, module in model.named_modules():\n",
    "        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=find_all_linear_names(model),\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64gK5tEB89qq"
   },
   "source": [
    "## Create PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jHNidtY689qq"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import Any, Dict\n",
    "import random\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "class LlavaNextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for LLaVa-NeXT. This class takes a HuggingFace Dataset as input.\n",
    "\n",
    "    Each row, consists of image path(png/jpg/jpeg) and ground truth data (json/jsonl/txt).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        split: str = \"train\",\n",
    "        sort_json_key: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.split = split\n",
    "        self.sort_json_key = sort_json_key\n",
    "\n",
    "        self.dataset = dataset[split]\n",
    "        self.dataset_length = len(self.dataset)\n",
    "        self.prompt = \"\"\"You are a system that checks if a photo meets passport photo requirements.\n",
    "Respond in exactly one of two ways:\n",
    "- \"Acceptable\"\n",
    "- \"Unacceptable: <One ore more criteria>\"\n",
    "\n",
    "Criteria to consider: \n",
    "Format: Head size must be just right and be in the centre of the frame\n",
    "Photo quality:  In sharp focus and clear. Neutral colour, natural skin tones, no red eyes.\n",
    "Lighting: Appropriate brightness and contrast.  Balanced lighting, no shadows or flash reflections on face.\n",
    "Eyes: Directly looking at the camera. Eyes open and clearly visible, no hair across the eyes.\n",
    "Pose: Face must be in the centre. Portrait style and tilted positions are not acceptable. The photograph must show both sides of the face evenly\n",
    "Background: Plain light-coloured (single-coloured) background. The photographed person must be shown alone with clear background\n",
    "Glasses: Eyes must be showed clearly with no flash reflections on glasses.  No tinted glasses. Frames must not cover any part of the eyes.\n",
    "Head coverings: Head coverings are not permitted except for religious reasons. Facial features from bottom of the chin to top of forehead and both sides of the face must be clearly shown.\n",
    "Facial Expression: Facial expression must be neutral. Mouth must be closed.\n",
    "\"\"\"\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((processor.image_processor.size[\"shortest_edge\"],   # 336\n",
    "                      processor.image_processor.size[\"shortest_edge\"]),\n",
    "                     interpolation=InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),                                # (3, H, W), 0-1 floats\n",
    "        ])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        record = self.dataset[idx]\n",
    "        instruction = self.prompt\n",
    "        output = record[\"output\"]\n",
    "\n",
    "        # Load and convert to RGB\n",
    "        image = record['image'].convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"instruction\": instruction,\n",
    "            \"output\": output\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmEj8Xjx89qr"
   },
   "source": [
    "Let's instantiate the PyTorch datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VG-74l9189qr"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('parquet', data_files={\"train\": \"./data/train/dataset.parquet\"})\n",
    "train_dataset = LlavaNextDataset(dataset,  split=\"train\", sort_json_key=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNm0Ui1a89qr"
   },
   "source": [
    "As always, it's important to check your data. Let's check the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "S1AiW6oa89qr",
    "outputId": "3efff66a-a781-4572-f45d-ee9d7d2a806c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[0.9490, 0.9529, 0.9529,  ..., 0.9529, 0.9451, 0.9451],\n",
      "         [0.9490, 0.9451, 0.9451,  ..., 0.9529, 0.9490, 0.9451],\n",
      "         [0.9490, 0.9451, 0.9451,  ..., 0.9569, 0.9529, 0.9490],\n",
      "         ...,\n",
      "         [0.9451, 0.9412, 0.9412,  ..., 0.0039, 0.0039, 0.0039],\n",
      "         [0.9451, 0.9412, 0.9255,  ..., 0.0039, 0.0039, 0.0000],\n",
      "         [0.9412, 0.9490, 0.8824,  ..., 0.0039, 0.0039, 0.0078]],\n",
      "\n",
      "        [[0.9569, 0.9529, 0.9569,  ..., 0.9569, 0.9529, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9529,  ..., 0.9569, 0.9529, 0.9490],\n",
      "         [0.9490, 0.9490, 0.9490,  ..., 0.9608, 0.9529, 0.9529],\n",
      "         ...,\n",
      "         [0.9569, 0.9490, 0.9608,  ..., 0.0000, 0.0039, 0.0078],\n",
      "         [0.9569, 0.9529, 0.9451,  ..., 0.0000, 0.0039, 0.0157],\n",
      "         [0.9608, 0.9608, 0.9137,  ..., 0.0039, 0.0039, 0.0078]],\n",
      "\n",
      "        [[0.9569, 0.9608, 0.9569,  ..., 0.9529, 0.9451, 0.9412],\n",
      "         [0.9608, 0.9569, 0.9529,  ..., 0.9490, 0.9451, 0.9451],\n",
      "         [0.9569, 0.9529, 0.9529,  ..., 0.9529, 0.9490, 0.9490],\n",
      "         ...,\n",
      "         [0.9529, 0.9529, 0.9647,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.9529, 0.9451, 0.9569,  ..., 0.0000, 0.0039, 0.0078],\n",
      "         [0.9569, 0.9608, 0.9294,  ..., 0.0039, 0.0078, 0.0118]]]), 'instruction': 'You are a system that checks if a photo meets passport photo requirements.\\nRespond in exactly one of two ways:\\n- \"Acceptable\"\\n- \"Unacceptable: <One ore more criteria>\"\\n\\nCriteria to consider: \\nFormat: Head size must be just right and be in the centre of the frame\\nPhoto quality:  In sharp focus and clear. Neutral colour, natural skin tones, no red eyes.\\nLighting: Appropriate brightness and contrast.  Balanced lighting, no shadows or flash reflections on face.\\nEyes: Directly looking at the camera. Eyes open and clearly visible, no hair across the eyes.\\nPose: Face must be in the centre. Portrait style and tilted positions are not acceptable. The photograph must show both sides of the face evenly\\nBackground: Plain light-coloured (single-coloured) background. The photographed person must be shown alone with clear background\\nGlasses: Eyes must be showed clearly with no flash reflections on glasses.  No tinted glasses. Frames must not cover any part of the eyes.\\nHead coverings: Head coverings are not permitted except for religious reasons. Facial features from bottom of the chin to top of forehead and both sides of the face must be clearly shown.\\nFacial Expression: Facial expression must be neutral. Mouth must be closed.\\n', 'output': 'Acceptable.'}\n"
     ]
    }
   ],
   "source": [
    "train_example = train_dataset[0]\n",
    "print(train_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLChlej489qr"
   },
   "source": [
    "## Define collate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NFxVN7OK89qr"
   },
   "outputs": [],
   "source": [
    "def train_collate_fn(examples):\n",
    "    images, texts = [], []\n",
    "\n",
    "    for ex in examples:\n",
    "        image       = ex[\"image\"]\n",
    "        instruction = ex[\"instruction\"]\n",
    "        output      = ex[\"output\"]\n",
    "\n",
    "        images.append(image)\n",
    "\n",
    "        conversation = [\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": instruction},\n",
    "            ]},\n",
    "            {\"role\": \"assistant\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": output},\n",
    "            ]},\n",
    "        ]\n",
    "        texts.append(processor.apply_chat_template(conversation))\n",
    "\n",
    "    batch = processor(\n",
    "        text=texts,\n",
    "        images=images,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "    return (\n",
    "        batch[\"input_ids\"],\n",
    "        batch[\"attention_mask\"],\n",
    "        batch[\"pixel_values\"],\n",
    "        batch[\"image_sizes\"],\n",
    "        batch[\"labels\"],\n",
    "    )\n",
    "\n",
    "def eval_collate_fn(examples):\n",
    "    # We only feed the prompt to the model, so we don't add assistant's turn\n",
    "    # Rather we indicate to `add_generation_prompt=True`\n",
    "\n",
    "    images = []\n",
    "    texts = []\n",
    "    answers = []\n",
    "    for example in examples:\n",
    "        image, instruction, output = example\n",
    "        images.append(image)\n",
    "\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": instruction},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "        texts.append(text_prompt)\n",
    "        answers.append(output)\n",
    "\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    image_sizes = batch[\"image_sizes\"]\n",
    "\n",
    "    return input_ids, attention_mask, pixel_values, image_sizes, answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCKx3T0I89qr"
   },
   "source": [
    "## Define PyTorch LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "B0vMexZs89qr"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as L\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "from nltk import edit_distance\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LlavaModelPLModule(L.LightningModule):\n",
    "    def __init__(self, config, processor, model, train_dataset):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "\n",
    "        self.batch_size = config.get(\"batch_size\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        input_ids, attention_mask, pixel_values, image_sizes, labels = batch\n",
    "\n",
    "        outputs = self.model(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            pixel_values=pixel_values,\n",
    "                            image_sizes=image_sizes,\n",
    "                            labels=labels\n",
    "                          )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "\n",
    "        input_ids, attention_mask, pixel_values, image_sizes, answers = batch\n",
    "\n",
    "        # autoregressively generate token IDs\n",
    "        generated_ids = self.model.generate(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                       pixel_values=pixel_values, image_sizes=image_sizes, max_new_tokens=MAX_LENGTH)\n",
    "        # turn them back into text, chopping of the prompt\n",
    "        # important: we don't skip special tokens here, because we want to see them in the output\n",
    "        predictions = self.processor.batch_decode(generated_ids[:, input_ids.size(1):], skip_special_tokens=True)\n",
    "\n",
    "        scores = []\n",
    "        for pred, answer in zip(predictions, answers):\n",
    "            pred = re.sub(r\"(?:(?<=>) | (?=</s_))\", \"\", pred)\n",
    "            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n",
    "\n",
    "            if self.config.get(\"verbose\", False) and len(scores) == 1:\n",
    "                print(f\"Prediction: {pred}\")\n",
    "                print(f\"    Answer: {answer}\")\n",
    "                print(f\" Normed ED: {scores[0]}\")\n",
    "\n",
    "        self.log(\"val_edit_distance\", np.mean(scores))\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # you could also add a learning rate scheduler if you want\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.config.get(\"lr\"))\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, collate_fn=train_collate_fn, batch_size=self.batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIr62vI-89qs"
   },
   "source": [
    "# Configure, train and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"max_epochs\": 1,\n",
    "          \"check_val_every_n_epoch\": 1,\n",
    "          \"gradient_clip_val\": 1.0,\n",
    "          \"accumulate_grad_batches\": 8,\n",
    "          \"lr\": 2e-5,\n",
    "          \"batch_size\": 1,\n",
    "          \"num_nodes\": 1,\n",
    "          \"warmup_steps\": 50,\n",
    "          \"result_path\": \"./result\",\n",
    "          \"verbose\": True,\n",
    "}\n",
    "\n",
    "model_module = LlavaModelPLModule(config, processor, model, train_dataset)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "        enable_checkpointing=False, \n",
    "        accelerator=\"gpu\",\n",
    "        devices=[0],\n",
    "        max_epochs=config.get(\"max_epochs\"),\n",
    "        accumulate_grad_batches=config.get(\"accumulate_grad_batches\"),\n",
    "        check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
    "        gradient_clip_val=config.get(\"gradient_clip_val\"),\n",
    "        precision=\"16-mixed\",\n",
    "        limit_val_batches=5,\n",
    "        num_sanity_val_steps=0\n",
    ")\n",
    "\n",
    "trainer.fit(model_module)\n",
    "\n",
    "\n",
    "peft_model = model_module.model\n",
    "\n",
    "# ✨ option A: adapters only (tiny file)\n",
    "peft_model.save_pretrained(\"./fine-tuned-weights/lr2e-5___\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
